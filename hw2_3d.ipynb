{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li-ziang/2025-Fall-MLG/blob/main/hw2_3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQbSsL3q-4TL"
      },
      "source": [
        "# Coding Practice 2 Graph Neural Networks with 3D Coordinates: An Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKD3Zomj-8Ve"
      },
      "source": [
        "Welcome to this hands-on exploration of Graph Neural Networks (GNNs) with a focus on 3D coordinates. In this assignment, we delve into the fascinating intersection of graph theory, neural networks, and geometric deep learning. You will have the opportunity to build and train GNNs that can effectively handle 3D structured data, a common occurrence in fields such as computational chemistry, material science, and computer vision.\n",
        "\n",
        "Understanding how to manipulate and process graph data that includes 3D spatial information is critical for developing models that can learn from the geometric relationships inherent in many real-world datasets. Throughout this assignment, you will tackle the unique challenges posed by 3D data, learn to implement GNNs that are invariant or equivariant to 3D rotations and translations, and apply your knowledge to solve problems that require an understanding of the underlying spatial structure.\n",
        "\n",
        "Get ready to enhance your machine learning toolkit with the capability to process 3D graph data, and prepare to unlock a new dimension of possibilities!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_wGdUF_NlJ"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "For a seamless execution of this notebook, ensure your Python environment is properly set up. Here's what you'll need:\n",
        "\n",
        "- **Python Version**: We recommend using Python 3.8 or higher.\n",
        "- **Required Packages**: Install the following libraries to delve into GNNs:\n",
        "  - `torch`\n",
        "  - `torch_geometric`\n",
        "  - `torch_scatter`\n",
        "  - `torch_sparse`\n",
        "  - `torchmetrics`\n",
        "  - `networkx`\n",
        "  - `numpy`\n",
        "  - `jupyter`\n",
        "  - `rdkit-pypi`\n",
        "  - `py3Dmol`\n",
        "  - `pandas`\n",
        "  - `seaborn`\n",
        "\n",
        "- **For Local Testing**: If you wish to visualize and run tests outside this notebook, please also install:\n",
        "  - `matplotlib`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kEfJebI_W-2"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install rdkit py3Dmol\n",
        "!pip install -q torchmetrics\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft19s8uR_ep0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import py3Dmol\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from google.colab import files\n",
        "from rdkit.Chem import AllChem, Crippen, QED, rdMolDescriptors, rdmolops\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Geometry.rdGeometry import Point3D\n",
        "from scipy.stats import ortho_group\n",
        "from torch.nn import BatchNorm1d, Linear, Module, ReLU, Sequential\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import rdkit.Chem as Chem\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Batch, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.utils import dense_to_sparse, remove_self_loops, to_dense_adj\n",
        "import torch_geometric.transforms as T\n",
        "from torch_scatter import scatter\n",
        "\n",
        "print(\"Torch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2susvfIBezh"
      },
      "source": [
        "## Graph Neural Networks in Chemistry\n",
        "\n",
        "In the realm of computational chemistry, graph neural networks (GNNs) have emerged as a powerful tool to model and predict the properties of molecules. Molecules can be naturally depicted as graphs, where atoms serve as nodes and chemical bonds act as edges. This representation is particularly advantageous for **Molecular Property Prediction**, where GNNs can learn from existing molecular data to predict physical and chemical properties crucial in drug discovery and material science.\n",
        "\n",
        "\n",
        "A striking illustration of GNN utility is in predicting molecular activities, which has direct implications for **drug discovery**. For instance, GNNs have been instrumental in identifying novel compounds with therapeutic potential by predicting their interaction with biological targets. A notable success story is the discovery of [**Halicin**](https://en.wikipedia.org/wiki/Halicin), a compound with promising antibacterial properties found through GNN-driven screening.\n",
        "\n",
        "\n",
        "## Delving into the QM9 Dataset\n",
        "\n",
        "The QM9 dataset is a comprehensive collection of around **130,000 small molecules** characterized by 19 different regression targets, making it a gold standard for evaluating GNNs in molecular property prediction. This dataset has gained prominence following its adoption by the [MoleculeNet](https://arxiv.org/abs/1703.00564) benchmark.\n",
        "\n",
        "Our focus will be on predicting the [homolumo gap](https://en.wikipedia.org/wiki/HOMO/LUMO), a quantum property that is indicative of a moleculeâ€™s reactivity. The homolumo gap refers to the energy difference between the highest occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital (LUMO), as depicted below:\n",
        "\n",
        "\n",
        "For our purposes, it's not necessary to delve into the quantum mechanics governing the homolumo gap. It suffices to understand that representing molecules as graphs with node features, edge features, and spatial coordinates allows us to leverage GNNs to predict such intricate properties from provided ground truth data.\n",
        "\n",
        "With this understanding, let's proceed to load the QM9 dataset and examine the structure of these molecular graphs, a process simplified by the utilities provided by PyG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUeows2tCDJU"
      },
      "outputs": [],
      "source": [
        "class SetTarget(object):\n",
        "    \"\"\"\n",
        "    This transform modifies the labels vector per data sample to only keep\n",
        "    the label for a specific target (there are 19 targets in QM9).\n",
        "\n",
        "    Note: for this practical, we allow the target to be set via the constructor.\n",
        "    \"\"\"\n",
        "    def __init__(self, target=0):  # Allow target to be set at instantiation\n",
        "        self.target = target\n",
        "\n",
        "    def __call__(self, data):\n",
        "        data.y = data.y[:, self.target]  # Use the target set in the constructor\n",
        "        return data\n",
        "\n",
        "class CompleteGraph(object):\n",
        "    \"\"\"\n",
        "    This transform adds all pairwise edges into the edge index per data sample,\n",
        "    then removes self loops, i.e., it builds a fully connected or complete graph.\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "# Define the target outside of the classes for flexibility\n",
        "target = 0\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    path = './qm9'\n",
        "\n",
        "    # Transforms which are applied during data loading:\n",
        "    # (1) Fully connect the graphs, (2) Select the target/label\n",
        "    transform = T.Compose([CompleteGraph(), SetTarget(target)])\n",
        "\n",
        "    # Load the QM9 dataset with the transforms defined\n",
        "    dataset = QM9(path, transform=transform)\n",
        "\n",
        "    # Normalize targets to have zero mean and unit variance\n",
        "    mean, std = dataset.data.y[:, target].mean(), dataset.data.y[:, target].std()\n",
        "    dataset.data.y[:, target] = (dataset.data.y[:, target] - mean) / std\n",
        "    mean, std = mean.item(), std.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnxqcufEULf"
      },
      "source": [
        "## Data Preparation and Dataset Splitting\n",
        "\n",
        "The comprehensive QM9 dataset encompasses over **130,000** molecular graphs, providing a rich ground for training robust models in molecular property prediction.\n",
        "\n",
        "For the scope of this assignment, we'll work with a curated subset of **4,500** molecular graphs. This subset size strikes a balance between computational manageability and sufficient data complexity. We will divide this subset into distinct training, validation, and test sets, each comprising 1,500 graphs. This partitioning allows for a thorough evaluation of our model's performance.\n",
        "\n",
        "Later in the assignment, you'll have the opportunity to scale your experiments to larger portions of the QM9 dataset, challenging your model with an even broader array of molecular structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOp3fjCBCEHY"
      },
      "outputs": [],
      "source": [
        "print(f\"Total number of samples available for selection: {len(dataset)}.\")\n",
        "\n",
        "# Split datasets (our 4.5K subset)\n",
        "train_dataset = dataset[:1500]\n",
        "val_dataset = dataset[1500:3000]\n",
        "test_dataset = dataset[3000:4500]\n",
        "print(f\"Dataset divisions established with {len(train_dataset)} samples for training, \" +\n",
        "      f\"{len(val_dataset)} samples for validation, and {len(test_dataset)} samples for testing.\")\n",
        "\n",
        "# Create dataloaders with batch size = 256\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwVjxXy2E6g2"
      },
      "outputs": [],
      "source": [
        "data = train_dataset[0] # one data sample, i.e. molecular graph\n",
        "print(\"One molecular graph contains:\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VGbtTeOFvKL"
      },
      "source": [
        "## Understanding the Graph Representation of Molecules\n",
        "\n",
        "In our dataset, each molecule is represented as a graph with specific attributes accessible via a `Data` object in PyTorch Geometric. Here's an overview of the graph features we'll work with:\n",
        "\n",
        "**Node Features (`data.x`)**: Each node (atom) in the graph has an 11-dimensional feature vector that includes information such as atom type, atomic number, and other chemical properties.\n",
        "\n",
        "**Edge Connectivity (`data.edge_index`)**: This is a tensor that defines which nodes (atoms) are connected to which, representing the bonds in the molecule.\n",
        "\n",
        "**Edge Features (`data.edge_attr`)**: For each edge (bond), a 4-dimensional vector describes the type of bond between the connected atoms using one-hot encoding.\n",
        "\n",
        "**Atomic Positions (`data.pos`)**: The 3D coordinates for each atom in the molecule are included, which will be crucial for our 3D graph neural network models.\n",
        "\n",
        "**Target Property (`data.y`)**: We're interested in predicting a single property of the molecule, such as its electric dipole moment, represented here as a scalar value.\n",
        "\n",
        "**Important Note**: In our approach, we consider fully-connected graphs, meaning every atom is connected to every other atom. The edge features will distinguish between actual chemical bonds and non-bonded pairs of atoms: real bonds are indicated by their bond type, while non-bonded pairs have zero vectors for their edge attributes.\n",
        "\n",
        "This setup allows our model to learn from both the chemical bonds and the spatial structure of the molecules, providing a comprehensive view of each molecule's potential properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOW4YK5bFTVY"
      },
      "outputs": [],
      "source": [
        "atom_count = data.x.shape[0]\n",
        "edge_count = data.edge_attr.shape[0]\n",
        "atom_features = data.x.shape[1]\n",
        "edge_features = data.edge_attr.shape[1]\n",
        "coordinate_dimensions = data.pos.shape[1]\n",
        "target_count = data.y.shape[0]\n",
        "\n",
        "print(\"\\nExploring the molecular graph:\")\n",
        "print(\"Number of atoms:\", atom_count)\n",
        "print(\"Number of bonds:\", edge_count)\n",
        "print(\"Features per atom:\", atom_features)\n",
        "print(\"Features per bond:\", edge_features)\n",
        "print(\"Dimensions of spatial coordinates per atom:\", coordinate_dimensions)\n",
        "print(\"Total targets to predict for the molecule:\", target_count)\n",
        "\n",
        "print(\"\\nUp next, we'll dive into constructing a Graph Neural Network using Message Passing to utilize these features for predicting molecular properties.\")\n",
        "print(\"The significance of the spatial coordinates will be covered in an upcoming section of this tutorial.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-tjBYAcNGlX"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "from torch.nn import Linear, BatchNorm1d, ReLU, Sequential, Module\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_scatter import scatter_add, scatter\n",
        "\n",
        "\n",
        "class MPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        super().__init__(aggr=aggr)  # Initialize the parent class with the aggregation method.\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # Message MLP: (2 * emb_dim + edge_dim) -> emb_dim\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2 * emb_dim + edge_dim, emb_dim),\n",
        "            BatchNorm1d(emb_dim),\n",
        "            ReLU(),\n",
        "            Linear(emb_dim, emb_dim),\n",
        "            BatchNorm1d(emb_dim),\n",
        "            ReLU()\n",
        "        )\n",
        "\n",
        "        # Update MLP: (2 * emb_dim) -> emb_dim\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2 * emb_dim, emb_dim),\n",
        "            BatchNorm1d(emb_dim),\n",
        "            ReLU(),\n",
        "            Linear(emb_dim, emb_dim),\n",
        "            BatchNorm1d(emb_dim),\n",
        "            ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr):\n",
        "        # Propagate messages using the defined message and update functions.\n",
        "        return self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, h_i, h_j, edge_attr):\n",
        "        # Constructs messages for each edge in the graph.\n",
        "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
        "        return self.mlp_msg(msg)\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size=None):\n",
        "        # Aggregates messages using the specified aggregation method.\n",
        "        return scatter_add(inputs, index, dim=self.node_dim, dim_size=dim_size)\n",
        "\n",
        "    def update(self, aggr_out, h):\n",
        "        # Updates node features by combining aggregated messages with initial node features.\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(emb_dim={self.emb_dim}, edge_dim={self.edge_dim}, aggr={self.aggr})'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M7N1ewEF--q"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class NaiveModel(Module):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"\n",
        "        Message Passing Neural Network (MPNN) model for graph property prediction.\n",
        "\n",
        "        Parameters:\n",
        "        - num_layers (int): Number of message passing layers.\n",
        "        - emb_dim (int): Embedding dimension for node features.\n",
        "        - in_dim (int): Dimension of initial node features.\n",
        "        - edge_dim (int): Dimension of edge features.\n",
        "        - out_dim (int): Dimension of output features.\n",
        "        \"\"\"\n",
        "        super(NaiveModel, self).__init__()\n",
        "\n",
        "        self.embedding = Linear(in_dim, emb_dim)\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            MPNNLayer(emb_dim, edge_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.readout = global_mean_pool\n",
        "        self.output = Linear(emb_dim, out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # Embed initial node features\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Apply MPNN layers with residual connections\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x, edge_index, edge_attr)\n",
        "\n",
        "        # Pool graph representations\n",
        "        x = self.readout(x, batch)\n",
        "\n",
        "        # Apply final output layer\n",
        "        return self.output(x).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVFbBpVzHIww"
      },
      "outputs": [],
      "source": [
        "def permute_graph(data, perm):\n",
        "    \"\"\"Helper function for permuting PyG Data object attributes consistently.\n",
        "    \"\"\"\n",
        "    # Permute the node attribute ordering\n",
        "    data.x = data.x[perm]\n",
        "    data.pos = data.pos[perm]\n",
        "    data.z = data.z[perm]\n",
        "    data.batch = data.batch[perm]\n",
        "\n",
        "    # Permute the edge index\n",
        "    adj = to_dense_adj(data.edge_index)\n",
        "    adj = adj[:, perm, :]\n",
        "    adj = adj[:, :, perm]\n",
        "    data.edge_index = dense_to_sparse(adj)[0]\n",
        "\n",
        "    # Note:\n",
        "    # (1) While we originally defined the permutation matrix P as only having\n",
        "    #     entries 0 and 1, its implementation via `perm` uses indexing into\n",
        "    #     torch tensors, instead.\n",
        "    # (2) It is cumbersome to permute the edge_attr, so we set it to constant\n",
        "    #     dummy values. For any experiments beyond unit testing, all GNN models\n",
        "    #     use the original edge_attr.\n",
        "\n",
        "    return data\n",
        "\n",
        "def permutation_invariance_unit_test(module, dataloader):\n",
        "    \"\"\"Unit test for checking whether a module (GNN model) is\n",
        "    permutation invariant.\n",
        "    \"\"\"\n",
        "    it = iter(dataloader)\n",
        "    data = next(it)\n",
        "\n",
        "    # Set edge_attr to dummy values (for simplicity)\n",
        "    data.edge_attr = torch.zeros(data.edge_attr.shape)\n",
        "\n",
        "    # Forward pass on original example\n",
        "    out_1 = module(data)\n",
        "\n",
        "    # Create random permutation\n",
        "    perm = torch.randperm(data.x.shape[0])\n",
        "    data = permute_graph(data, perm)\n",
        "\n",
        "    # Forward pass on permuted example\n",
        "    out_2 = module(data)\n",
        "\n",
        "    # Check whether output varies after applying transformations\n",
        "    return torch.allclose(out_1, out_2, atol=1e-04)\n",
        "\n",
        "\n",
        "def permutation_equivariance_unit_test(module, dataloader):\n",
        "    \"\"\"Unit test for checking whether a module (GNN layer) is\n",
        "    permutation equivariant.\n",
        "    \"\"\"\n",
        "    it = iter(dataloader)\n",
        "    data = next(it)\n",
        "\n",
        "    # Set edge_attr to dummy values (for simplicity)\n",
        "    data.edge_attr = torch.zeros(data.edge_attr.shape)\n",
        "\n",
        "    # Forward pass on original example\n",
        "    out_1 = module(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Create random permutation\n",
        "    perm = torch.randperm(data.x.shape[0])\n",
        "    data = permute_graph(data, perm)\n",
        "\n",
        "    # Forward pass on permuted example\n",
        "    out_2 = module(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Check whether output varies after applying transformations\n",
        "    return torch.allclose(out_1[perm], out_2, atol=1e-04)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nLSXiSRHOoZ"
      },
      "outputs": [],
      "source": [
        "layer = MPNNLayer(emb_dim=11, edge_dim=4)\n",
        "model = NaiveModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Permutation invariance unit test for MPNN model\n",
        "print(f\"Is {type(model).__name__} permutation invariant? --> {permutation_invariance_unit_test(model, dataloader)}!\")\n",
        "\n",
        "# Permutation equivariance unit for MPNN layer\n",
        "print(f\"Is {type(layer).__name__} permutation equivariant? --> {permutation_equivariance_unit_test(layer, dataloader)}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRaCXi0zHSWE"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch)\n",
        "        loss = F.mse_loss(predictions, batch.y)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader.dataset)\n",
        "    return average_loss\n",
        "\n",
        "def eval(model, loader, device):\n",
        "    model.eval()\n",
        "    error = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(data)\n",
        "            # Mean Absolute Error using std (computed when preparing data)\n",
        "            error += ((y_pred - data.y) * std).abs().sum().item()\n",
        "    return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "def run_experiment(model, model_name, train_loader, val_loader, test_loader, n_epochs=100):\n",
        "    print(f\"Running experiment for {model_name}, training on {len(train_loader.dataset)} samples for {n_epochs} epochs.\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"\\ndevice:\", device, \"\\nModel architecture:\")\n",
        "    print(model)\n",
        "    total_params = sum(np.prod(p.size()) for p in model.parameters())\n",
        "    print(f'Total parameters: {total_params}')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.9, patience=5, min_lr=0.00001)\n",
        "\n",
        "    print(\"\\nStart training:\")\n",
        "    best_val_error = float('inf')\n",
        "    best_test_error = float('inf')\n",
        "    perf_per_epoch = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train(model, train_loader, optimizer, device)\n",
        "        val_error = eval(model, val_loader, device)\n",
        "\n",
        "        if val_error < best_val_error:\n",
        "            best_val_error = val_error\n",
        "            best_test_error = eval(model, test_loader, device)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch: {epoch:03d}, LR: {current_lr:.6f}, Loss: {loss:.7f}, '\n",
        "                  f'Val MAE: {val_error:.7f}, Test MAE: {best_test_error:.7f}')\n",
        "\n",
        "        scheduler.step(val_error)\n",
        "        perf_per_epoch.append((best_test_error, val_error, epoch, model_name))\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    train_time_minutes = elapsed_time / 60\n",
        "    print(f\"\\nDone! Training took {train_time_minutes:.2f} mins. \"\n",
        "          f\"Best validation MAE: {best_val_error:.7f}, corresponding test MAE: {best_test_error:.7f}.\")\n",
        "\n",
        "    return best_val_error, best_test_error, train_time_minutes, perf_per_epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIucvDE6N7H4"
      },
      "outputs": [],
      "source": [
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycfNUbNCHXQP"
      },
      "outputs": [],
      "source": [
        "model = NaiveModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "results[model_name] = (best_val_error, test_error, train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHfSewxHHbbE"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTdLKhz5TH58"
      },
      "source": [
        "## Task 1: Implement a Message Passing Neural Network Utilizing Atom Coordinates as Node Features [3 pts]\n",
        "\n",
        "The baseline MPNN, labeled `NaiveModel`, does not consider the atom coordinates and relies solely on node features for message passing. This approach misses out on critical 3D structural information that could be pivotal for predicting the target property.\n",
        "\n",
        "The objective of your first task is to enhance the `NaiveModel` by integrating the atom coordinates with the node features.\n",
        "\n",
        "While we have outlined the structure of the `PositionModel` class, certain sections marked `TODO` remain incomplete and require your implementation.\n",
        "\n",
        "Keep in mind that the 3D atom positions can be accessed via `data.pos`. At this stage, a straightforward approach, such as concatenating or summing the coordinates with the features, would suffice to make progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWg2vk64O-Ip"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "class PositionModel(NaiveModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"Initializes a PositionModel which is an extension of NaiveModel that\n",
        "        includes atom coordinates in addition to the node features.\n",
        "\n",
        "        Parameters:\n",
        "            num_layers (int): The number of message passing layers.\n",
        "            emb_dim (int): The embedding dimension of node features.\n",
        "            in_dim (int): The dimension of initial node features.\n",
        "            edge_dim (int): The dimension of edge features.\n",
        "            out_dim (int): The dimension of the model output.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Define input linear layer\n",
        "        # Input layer that projects initial node features and coordinates\n",
        "\n",
        "        # TODO: Define message passing layers\n",
        "        # self.layers =\n",
        "\n",
        "        # Define global pooling function (mean pooling)\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Output layer that predicts the graph property\n",
        "        self.output = Linear(emb_dim, out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"Performs a forward pass on the graph data.\n",
        "\n",
        "        Parameters:\n",
        "            data (PyG.Data): A batch of graphs in PyG format.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output predictions for each graph in the batch.\n",
        "        \"\"\"\n",
        "        # TODO: decide how to handle the input information\n",
        "        # Combine node features with atom positions\n",
        "\n",
        "        # Apply message passing layers with residual connections\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h, data.edge_index, data.edge_attr)\n",
        "\n",
        "        # Apply global mean pooling to get graph-level representation\n",
        "        h_graph = self.pool(h, data.batch)\n",
        "\n",
        "        # Predict the target property for each graph\n",
        "        out = self.output(h_graph)\n",
        "\n",
        "        # Flatten output for consistency\n",
        "        return out.view(-1)\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLDH2yOsUXzj"
      },
      "outputs": [],
      "source": [
        "layer = MPNNLayer(emb_dim=11, edge_dim=4, aggr='add')\n",
        "model = PositionModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Permutation invariance unit test for Position model\n",
        "print(f\"Is {type(model).__name__} permutation invariant? --> {permutation_invariance_unit_test(model, dataloader)}!\")\n",
        "\n",
        "# Permutation equivariance unit for MPNN layer\n",
        "print(f\"Is {type(layer).__name__} permutation equivariant? --> {permutation_equivariance_unit_test(layer, dataloader)}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8qofZl7Ul9D"
      },
      "outputs": [],
      "source": [
        "model = PositionModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "results[model_name] = (best_val_error, test_error, train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky0M18QNUsiq"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWcTWr8Xg_4D"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"pos_gnn.pth\")\n",
        "print(\"Saved PyTorch Model State to pos_gnn.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snXHecpdWmEW"
      },
      "source": [
        "Upon successful implementation of the `PositionModel`, an interesting observation emerges:\n",
        "the model's performance is comparable or slightly inferior to that of the basic `NaiveModel`.\n",
        "\n",
        "This performance pattern suggests that the `PositionModel` may not be effectively leveraging the\n",
        "3D structural information during its computations.\n",
        "\n",
        "In the following sections, we aim to delve deeper into the reasons behind this phenomenon and seek\n",
        "to establish a more methodical approach to utilizing 3D structural data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7LFvNhDXLKV"
      },
      "source": [
        "## Understanding Invariance to 3D Symmetries\n",
        "\n",
        "The `PositionModel` did not outperform the `NaiveModel` as expected, despite its use of additional coordinate data. Before you worry about the implications of your findings, it's important to delve into the concept of 3D symmetries.\n",
        "\n",
        "### Understanding Geometric Invariance\n",
        "\n",
        "Molecular graphs come with 3D atomic coordinates. An important aspect we've not discussed yet is the relativity of these coordinates. They are not static; instead, they are determined in relation to a reference point.\n",
        "\n",
        "Consider this visual of a molecule floating in 3D space, showcasing how it rotates and translates:\n",
        "\n",
        "<!-- Image placeholder for molecule GIF -->\n",
        "\n",
        "Despite the constant movement in coordinates, the intrinsic properties of the molecule remain constant. They are invariant to any changes in position or orientation in space.\n",
        "\n",
        "This part of our study will focus on creating GNN layers and models that honor this invariance.\n",
        "\n",
        "### Defining the Formalism\n",
        "\n",
        "We will now define the concept of invariance within GNNs using matrix representation.\n",
        "\n",
        "- Define $\\mathbf{H} \\in \\mathbb{R}^{n \\times d}$ as the feature matrix for a molecule's graph, with $n$ atoms and $d$ features per atom.\n",
        "- Define $\\mathbf{X} \\in \\mathbb{R}^{n \\times 3}$ as the coordinate matrix for the graph's atoms.\n",
        "- Define $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ as the adjacency matrix indicating connections between atoms.\n",
        "- Define $\\mathbf{F}(\\mathbf{H}, \\mathbf{X}, \\mathbf{A})$ as a GNN layer that processes these matrices to update node features.\n",
        "- Define $f(\\mathbf{H}, \\mathbf{X}, \\mathbf{A})$ as a GNN model that uses the matrices to predict a property at the graph level.\n",
        "\n",
        "We have updated the notations for the GNN layer $\\mathbf{F}$ and the GNN model $\\mathbf{f}$ to incorporate $\\mathbf{X}$, representing the node coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Z5exnGWY2y"
      },
      "outputs": [],
      "source": [
        "def random_orthogonal_matrix(dim=3):\n",
        "  \"\"\"Helper function to build a random orthogonal matrix of shape (dim, dim)\n",
        "  \"\"\"\n",
        "  Q = torch.tensor(ortho_group.rvs(dim=dim)).float()\n",
        "  return Q\n",
        "\n",
        "\n",
        "def rot_trans_invariance_unit_test(module, dataloader):\n",
        "    \"\"\"Unit test for checking whether a module (GNN model/layer) is\n",
        "    rotation and translation invariant.\n",
        "    \"\"\"\n",
        "    it = iter(dataloader)\n",
        "    data = next(it)\n",
        "\n",
        "    if isinstance(module, NaiveModel):\n",
        "        out_1 = module(data)\n",
        "    else: # if ininstance(module, MessagePassing):\n",
        "        out_1 = module(data.x, data.pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "    Q = random_orthogonal_matrix(dim=3)\n",
        "    t = torch.rand(3)\n",
        "    data.pos = data.pos @ Q + t\n",
        "\n",
        "    if isinstance(module, NaiveModel):\n",
        "        out_2 = module(data)\n",
        "    else: # if ininstance(module, MessagePassing):\n",
        "        out_2 = module(data.x, data.pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "    return torch.allclose(out_1, out_2, atol=1e-04)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieBHPuBJXrY2"
      },
      "outputs": [],
      "source": [
        "# Instantiate temporary model, layer, and dataloader for unit testing\n",
        "model = PositionModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Rotation and translation invariance unit test for MPNN model\n",
        "print(f\"Is {type(model).__name__} rotation and translation invariant? --> {rot_trans_invariance_unit_test(model, dataloader)}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inii4v7sYRxW"
      },
      "source": [
        "## Task 2: Construct a Message Passing Layer and MPNN Model with 3D Invariance (3 pts)\n",
        "\n",
        "Task Objective:\n",
        "Develop a Message Passing Layer (`InvariantLayer`) and an MPNN Model (`InvariantModel`) that are inherently invariant to transformations in 3D space, such as rotations and translations.\n",
        "\n",
        "Background:\n",
        "The original MPNN model (`NaiveModel`) did not consider atom coordinates, relying exclusively on node features. The subsequent model (`PositionModel`) included coordinates but failed to maintain invariance to spatial transformations, missing out on the robustness required for accurate property prediction regardless of the molecule's orientation or position.\n",
        "\n",
        "Your Role:\n",
        "Create the `InvariantLayer` that effectively integrates atom coordinates and node features. This new layer should contribute to an MPNN Model that is resilient to changes in the molecular structure's spatial configuration.\n",
        "\n",
        "We provide the skeletal structure of `InvariantLayer` with sections marked for your implementation. The `InvariantModel` is pre-defined and will utilize your newly created layer.\n",
        "\n",
        "Hints for Implementation:\n",
        "- Rethink the integration of coordinate data in message construction, rather than mixing it directly with node features.\n",
        "- Focus on deriving a property from coordinate pairs that remains unchanged with spatial movements of the molecule.\n",
        "- Utilize the naming convention in `propagate()` to map tensors to their respective nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezsdAI2dXsnx"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "class InvariantLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"Initializes an MPNN layer that accounts for 3D coordinate invariance.\n",
        "\n",
        "        Args:\n",
        "            emb_dim (int): Size of each embedding vector.\n",
        "            edge_dim (int): Size of each edge feature vector.\n",
        "            aggr (str): Type of aggregation ('add', 'mean', or 'max').\n",
        "        \"\"\"\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        # TODO: Define the mlp_msg\n",
        "        # Message function network\n",
        "\n",
        "        # Update function network\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        \"\"\"Forward pass for generating updated node features.\"\"\"\n",
        "        #TODO: implement forward\n",
        "        pass\n",
        "\n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):\n",
        "        \"\"\"Computes messages for each edge in the graph.\"\"\"\n",
        "        # TODO: Implement message\n",
        "        pass\n",
        "\n",
        "    def update(self, aggr_out, h):\n",
        "        \"\"\"Updates node features after message aggregation.\"\"\"\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(emb_dim={}, edge_dim={}, aggr={})'.format(\n",
        "            self.__class__.__name__, self.emb_dim, self.edge_dim, self.aggr)\n",
        "\n",
        "class InvariantModel(NaiveModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"\n",
        "        Constructs a graph neural network model that ensures invariance to 3D coordinate transformations.\n",
        "\n",
        "        The model is designed to process graph data where each node is associated with both\n",
        "        features and 3D spatial coordinates, and it produces a prediction invariant to rotations\n",
        "        and translations of the coordinate space.\n",
        "\n",
        "        Parameters:\n",
        "            num_layers (int): The count of message-passing layers within the network.\n",
        "            emb_dim (int): The dimensionality of the embedding space for node features.\n",
        "            in_dim (int): The size of the input feature vector for each node.\n",
        "            edge_dim (int): The size of the feature vector for each edge.\n",
        "            out_dim (int): The size of the output vector; set to 1 for scalar predictions.\n",
        "        \"\"\"\n",
        "        # Call the constructor of the parent NaiveModel class\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature transformation layer to embed input node features into a higher-dimensional space\n",
        "        self.embedding = Linear(in_dim, emb_dim)\n",
        "\n",
        "        # Construct a sequence of graph convolution layers that are invariant to spatial transformations\n",
        "        self.layers = torch.nn.ModuleList([InvariantLayer(emb_dim, edge_dim, aggr='add') for _ in range(num_layers)])\n",
        "\n",
        "        # Define a pooling operation that aggregates node embeddings across the graph to form a graph-level representation\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Final linear layer that maps the graph-level representation to the prediction space\n",
        "        self.output = Linear(emb_dim, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns:\n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "        h = self.embedding(data.x)\n",
        "\n",
        "        for conv in self.layers:\n",
        "            h = h + conv(h, data.pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "        h_graph = self.pool(h, data.batch)\n",
        "\n",
        "        out = self.output(h_graph)\n",
        "        return out.view(-1)\n",
        "\n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYsPgd0VZl2D"
      },
      "outputs": [],
      "source": [
        "layer = InvariantLayer(emb_dim=11, edge_dim=4, aggr='add')\n",
        "model = InvariantModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(f\"Is {type(model).__name__} rotation and translation invariant? --> {rot_trans_invariance_unit_test(model, dataloader)}!\")\n",
        "\n",
        "print(f\"Is {type(layer).__name__} rotation and translation invariant? --> {rot_trans_invariance_unit_test(layer, dataloader)}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT29S2zzZpyK"
      },
      "outputs": [],
      "source": [
        "model = InvariantModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model,\n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Invariant Layers)\",\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "results[model_name] = (best_val_error, test_error, train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrpLKa0DaEGr"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJqvRVkZhN5g"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"invar_gnn.pth\")\n",
        "print(\"Saved PyTorch Model State to invar_gnn.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRvJA9EuagVh"
      },
      "source": [
        "You've progressed from the basic NaiveModel through a simple\n",
        "coordinate-inclusive PositionModel, to the geometrically sophisticated\n",
        "InvariantMPNN model. Next, we'll explore even deeper into the geometric\n",
        "aspects of molecular structures to extract more valuable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwlkXBnJbGix"
      },
      "source": [
        "## Message Passing with Equivariance to 3D Rotations and Translations\n",
        "\n",
        "Following our exploration of invariance to 3D rotation and translation, we now turn our attention to developing a GNN for molecular property prediction that leverages message passing layers with equivariance to these transformations.\n",
        "\n",
        "### The Importance of Geometric Equivariance\n",
        "\n",
        "We draw a parallel between permutation symmetries in GNNs, translation symmetries in ConvNets for 2D images, and now geometric equivariance in molecular models.\n",
        "\n",
        "#### Permutation Symmetry in GNNs and DeepSets\n",
        "\n",
        "Previously, we discussed how a GNN layer should be permutation equivariant, meaning the order of nodes should not affect the output, whereas the overall GNN model should be permutation invariant for graph-level predictions. This design allows the model to capture the relational structure of the graph. DeepSets, another permutation invariant model, can be used for graph-level predictions but may not capture the relational intricacies as effectively as GNNs, which use equivariant layers to build complex node representations.\n",
        "\n",
        "#### Translation Symmetry in ConvNets for 2D Images\n",
        "\n",
        "Similarly, ConvNets for image processing are translation invariant overall but composed of translation equivariant convolution filters. These filters detect features like edges and patterns regardless of their position in the input space, allowing the ConvNet to build hierarchical features and learn complex visual concepts.\n",
        "\n",
        "In conclusion, designing GNN layers that are equivariant to 3D rotations and translations could allow us to better capture the geometric structure of molecules and enhance model performance, just as equivariant layers in ConvNets allow for the extraction of complex visual patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So7AjQy3cBEZ"
      },
      "source": [
        "### Understanding Equivariance in GNNs\n",
        "\n",
        "We've established the significance of creating GNN layers that are equivariant to 3D rotations and translations. Now, let's define this concept with a mathematical framework.\n",
        "\n",
        "- Consider a matrix $\\mathbf{H} \\in \\mathbb{R}^{n \\times d}$ representing node features in a molecular graph, where $n$ is the number of nodes, and each row $h_i$ denotes the feature vector of dimension $d$ for node $i$.\n",
        "- Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times 3}$ represent the coordinates of nodes in the molecular graph, with each row $x_i$ corresponding to the 3D coordinates of node $i$.\n",
        "- The adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ indicates the connections between nodes, with $a_{ij}$ marking the link between nodes $i$ and $j$.\n",
        "- A GNN layer $\\mathbf{F}(\\mathbf{H}, \\mathbf{X}, \\mathbf{A}): \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{n \\times 3} \\times \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}^{n \\times d}\\times \\mathbb{R}^{n \\times 3}$ accepts node features, coordinates, and the adjacency matrix to return updated node features and coordinates.\n",
        "- The GNN model $f(\\mathbf{H}, \\mathbf{X}, \\mathbf{A}): \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{n \\times 3} \\times \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}$ computes the graph-level property from these inputs.\n",
        "\n",
        "The GNN model comprises several layers $\\mathbf{F}^{\\ell}(\\mathbf{H}^{\\ell}, \\mathbf{X}^{\\ell}, \\mathbf{A})$ that are equivariant to rotations and translations.\n",
        "\n",
        "### Distinction from Invariant Message Passing\n",
        "\n",
        "Equivariant message passing differs from invariant message passing because it updates not only the node features but also their coordinates:\n",
        "\n",
        "\\[\n",
        "\\mathbf{H}^{\\ell+1}, \\mathbf{X}^{\\ell+1} = \\mathbf{F}^{\\ell} (\\mathbf{H}^{\\ell}, \\mathbf{X}^{\\ell}, \\mathbf{A}).\n",
        "\\]\n",
        "\n",
        "This method is especially useful when modeling dynamical systems where node coordinates change due to intermolecular forces.\n",
        "\n",
        "Note these nuances regarding equivariant message passing layers $\\mathbf{F}$:\n",
        "- Updated node coordinates $\\mathbf{X'}$ are equivariant to the 3D transformations of the initial coordinates $\\mathbf{X}$.\n",
        "- Updated node features $\\mathbf{H'}$ remain invariant to the 3D transformations of $\\mathbf{X}$, similar to invariant message passing.\n",
        "- The overall MPNN model $f$ is invariant to 3D transformations because it predicts a single scalar quantity that does not change with the atoms' coordinate transformations.\n",
        "\n",
        "The final prediction of the GNN model is based on the graph embedding derived from the final node features after multiple layers of message passing, disregarding the final node coordinates.\n",
        "\n",
        "We aim to explore how using equivariant message passing layers can enhance a GNN model that is invariant to 3D symmetries.\n",
        "\n",
        "Let's proceed with our exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlZ5bkiycsif"
      },
      "source": [
        "## Task 3: Develop an Equivariant Message Passing Layer [4pts]\n",
        "\n",
        "**Objective:** Create a message passing layer that updates both the node features and coordinates, ensuring equivariance to 3D rotations and translations for a given molecular graph.\n",
        "\n",
        "**Guidance:**\n",
        "- The layer should handle both node features and coordinates, outputting a tuple that contains the updated versions of these elements.\n",
        "- Implement `message()`, `aggregate()`, and `update()` functions to work with these tuples, considering the distinct nature of invariant and equivariant quantities.\n",
        "- Invariant quantities should remain unchanged under 3D transformations, while equivariant quantities should change correspondingly with the coordinates.\n",
        "\n",
        "**Considerations:**\n",
        "- There are various strategies to achieve this, and simple replication of existing solutions from libraries like PyG is not acceptable.\n",
        "- Avoid trivial solutions such as leaving the coordinates unchanged. The goal is to design a coordinate message function that intelligently aggregates information from neighboring nodes' coordinates, adhering to 3D symmetries.\n",
        "\n",
        "The challenge lies in engineering a method for node coordinate updates through message passing that honors the principles of 3D equivariance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctQ-WmxjaV-Y"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "class EquivariantLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"\n",
        "        An MPNN layer that maintains equivariance to 3D geometric transformations.\n",
        "\n",
        "        Parameters:\n",
        "            emb_dim (int): The size of the hidden feature dimension.\n",
        "            edge_dim (int): The dimensionality of edge features.\n",
        "            aggr (str): The aggregation function to use.\n",
        "        \"\"\"\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # TODO: Define the essential variables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        Conducts a message passing update on node features and coordinates.\n",
        "\n",
        "        Parameters:\n",
        "            h (Tensor): Initial node features.\n",
        "            pos (Tensor): Node positions.\n",
        "            edge_index (LongTensor): Edge indices.\n",
        "            edge_attr (Tensor): Edge attributes.\n",
        "\n",
        "        Returns:\n",
        "            A tuple of updated node features and positions.\n",
        "        \"\"\"\n",
        "        # Message passing with updated coordinates\n",
        "        # TODO: implement forward\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):\n",
        "        \"\"\"\n",
        "        Generates messages for each node based on neighboring nodes and edge attributes.\n",
        "\n",
        "        Parameters:\n",
        "            h_i (Tensor): The features of the destination nodes.\n",
        "            h_j (Tensor): The features of the source nodes.\n",
        "            pos_i (Tensor): Positions of destination nodes.\n",
        "            pos_j (Tensor): Positions of source nodes.\n",
        "            edge_attr (Tensor): Edge attributes.\n",
        "\n",
        "        Returns:\n",
        "            A tuple of message vectors and position updates.\n",
        "        \"\"\"\n",
        "        # TODO: implement message\n",
        "        # Determine message and position changes\n",
        "        pass\n",
        "\n",
        "    def aggregate(self, inputs, index):\n",
        "        \"\"\"\n",
        "        Aggregates messages from neighboring nodes using the specified method.\n",
        "\n",
        "        Parameters:\n",
        "            inputs (tuple): Messages and positional updates from source nodes.\n",
        "            index (Tensor): Indices of the source nodes.\n",
        "\n",
        "        Returns:\n",
        "            A tuple of aggregated messages and position updates.\n",
        "        \"\"\"\n",
        "        # TODO: implement aggregate\n",
        "        # Unpack inputs and aggregate\n",
        "        pass\n",
        "\n",
        "\n",
        "    def update(self, aggr_out, h, pos):\n",
        "        \"\"\"\n",
        "        Updates node features using aggregated messages and initial features.\n",
        "\n",
        "        Parameters:\n",
        "            aggr_out (tuple): Aggregated messages and position updates.\n",
        "            h (Tensor): Initial node features.\n",
        "\n",
        "        Returns:\n",
        "            Updated node features after applying the update MLP.\n",
        "        \"\"\"\n",
        "        # TODO: implement update\n",
        "        # Extract aggregated results and apply updates\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "\n",
        "class EquivariantModel(NaiveModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"\n",
        "        Constructs an MPNN model that predicts graph properties while considering node features and spatial coordinates.\n",
        "\n",
        "        Parameters:\n",
        "            num_layers (int): The number of message passing layers in the model.\n",
        "            emb_dim (int): The size of the hidden feature dimension.\n",
        "            in_dim (int): The dimensionality of the initial node features.\n",
        "            edge_dim (int): The dimensionality of edge features.\n",
        "            out_dim (int): The dimension of the model's output.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = Linear(in_dim, emb_dim)  # Linear transformation for input features\n",
        "        self.layers = torch.nn.ModuleList([EquivariantLayer(emb_dim, edge_dim, aggr='add') for _ in range(num_layers)])  # Equivariant layers\n",
        "        self.pool = global_mean_pool  # Mean pooling layer\n",
        "        self.output = Linear(emb_dim, out_dim)  # Prediction layer\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Feeds data through the model to generate predictions.\n",
        "\n",
        "        Parameters:\n",
        "            data (Data): The input graph data.\n",
        "\n",
        "        Returns:\n",
        "            The predicted property for each graph in the batch.\n",
        "        \"\"\"\n",
        "        h = self.embedding(data.x)\n",
        "        pos = data.pos\n",
        "\n",
        "        for conv in self.layers:\n",
        "            h, pos = conv(h, pos, data.edge_index, data.edge_attr)  # Update features and positions\n",
        "\n",
        "        pooled_features = self.pool(h, data.batch)  # Pool features to graph-level\n",
        "\n",
        "        return self.output(pooled_features).view(-1)  # Predict graph properties\n",
        "\n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQRx1k-VdZFb"
      },
      "outputs": [],
      "source": [
        "def rot_trans_equivariance_unit_test(module, dataloader):\n",
        "    \"\"\"Unit test for checking whether a module (GNN layer) is\n",
        "    rotation and translation equivariant.\n",
        "    \"\"\"\n",
        "    it = iter(dataloader)\n",
        "    data = next(it)\n",
        "\n",
        "    # Original forward pass\n",
        "    out_1, pos_1 = module(data.x, data.pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Create random rotation and translation\n",
        "    Q = random_orthogonal_matrix(dim=3)\n",
        "    t = torch.rand(3)\n",
        "\n",
        "    # Apply rotation and translation\n",
        "    rotated_translated_pos = (data.pos @ Q.T) + t\n",
        "\n",
        "    # Forward pass on rotated + translated example\n",
        "    out_2, pos_2 = module(data.x, rotated_translated_pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Check whether output node features are the same (they should be equivariant)\n",
        "    feature_equivariance = torch.allclose(out_1, out_2, atol=1e-4)\n",
        "\n",
        "    # Check whether output positions are rotated and translated versions of pos_1\n",
        "    # Since pos_2 = Q * pos_1 + t, we should have pos_2 - t = Q * pos_1\n",
        "    pos_equivariance = torch.allclose((pos_2 - t), (pos_1 @ Q.T), atol=1e-4)\n",
        "\n",
        "    return feature_equivariance and pos_equivariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY_QzTM5dd8C"
      },
      "outputs": [],
      "source": [
        "# Instantiate the EquivariantLayerh appropriate dimensions\n",
        "layer = EquivariantLayer(emb_dim=11, edge_dim=4, aggr='add')\n",
        "\n",
        "# Instantiate the EquivariantModel with a specified number of layers and dimensions\n",
        "model = EquivariantModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Rotation and translation invariance unit test for MPNN model\n",
        "print(f\"Is {type(model).__name__} rotation and translation invariant? --> {rot_trans_invariance_unit_test(model, dataloader)}!\")\n",
        "\n",
        "# Rotation and translation invariance unit test for MPNN layer\n",
        "print(f\"Is {type(layer).__name__} rotation and translation equivariant? --> {rot_trans_equivariance_unit_test(layer, dataloader)}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGCdYjvfqzR"
      },
      "source": [
        "We have successfully created the `EquivariantLayer` and `EquivariantModel`, validating their 3D rotation and translation equivariance through theory and tests.\n",
        "\n",
        "Now, it's time to test our most advanced model that incorporates geometric principles.\n",
        "\n",
        "## Train and Test the EquivariantModel\n",
        "Train your `EquivariantModel` and evaluate its performance. Then, think about the outcomes in comparison to the earlier models: the basic `NaiveModel`, the `PositionModel` with straightforward coordinate usage, and the `InvariantModel`. Determine whether the new model shows superior performance and whether the improvement is substantial or marginal.\n",
        "\n",
        "\n",
        "For a fair comparison, configure the `EquivariantMPNNModel` with four message passing layers and a hidden dimension size of 64, aligning with the previous `NaiveModel`, `PositionModel`, and `InvariantModel` configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEafADaTdeOG"
      },
      "outputs": [],
      "source": [
        "model = EquivariantModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model,\n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\",\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "results[model_name] = (best_val_error, test_error, train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvNgj-QQfzyg"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED1xvJbxgMb3"
      },
      "source": [
        "Great job! You've progressed through various models, starting with the basic `NaiveModel`, moving to the elementary implementation of coordinate information in `PositionModel`, advancing to a geometrically informed `InvariantModel`, and culminating with the `EquivariantModel`. This latest model maintains **invariance to 3D rotations and translations** and integrates **message passing layers that are equivariant** to these spatial transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jce78pJAf1oL"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"equ_gnn.pth\")\n",
        "print(\"Saved PyTorch Model State to equ_gnn.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXrjBh3VhcMn"
      },
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "Ensure you've thoroughly tested your code locally before submitting it for evaluation. Your submission to Gradescope should be a zip file containing specific files related to your solution and the trained models.\n",
        "\n",
        "### Submission Checklist:\n",
        "\n",
        "Ensure your zip file contains the following items:\n",
        "\n",
        "1. **Notebook File**:\n",
        "   - `hw2_3d.ipynb`: The Jupyter notebook containing all your code and answers.\n",
        "\n",
        "2. **Model Files**:\n",
        "   - `pos_gnn.pth`: The saved model file for the Node GCN model.\n",
        "   - `invar_gnn.pth`: The saved model file for the Graph GCN model.\n",
        "   - `equ_gnn.path`: The saved model file for the Graph GNN model.\n",
        "\n",
        "All model files should adhere to the structure defined within your notebook.\n",
        "\n",
        "### Submission Instructions:\n",
        "\n",
        "- **File Format**: Submit all your files in a **ZIP** format.\n",
        "- **File Structure**: Avoid including a root directory in the zip file. Ensure all your files are compressed directly without a containing folder.\n",
        "- **Validation**: Before submitting, verify your code runs as expected and all outputs align with anticipated results.\n",
        "  \n",
        "### Additional Notes:\n",
        "\n",
        "- Ensure your models maintain the same structure as defined within your notebook.\n",
        "- Be mindful of ensuring all necessary components are included to avoid discrepancies during the evaluation process.\n",
        "\n",
        "**CRUCIAL**: It's paramount to test your zip file in a fresh environment to confirm it runs seamlessly and to verify all essential components are included.\n",
        "\n",
        "Best of luck with your submission!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}